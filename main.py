import os, shutil, uuid, asyncio, re, requests, base64
from faster_whisper import WhisperModel
from flask import Flask, request, jsonify, send_file, render_template
from deep_translator import GoogleTranslator
import edge_tts

ffmpeg_path = shutil.which("ffmpeg")
if ffmpeg_path:
    os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_path

app = Flask(__name__, template_folder=".")

print("Loading Whisper model...")
model = WhisperModel("tiny", device="cpu", compute_type="int8")
print("Model ready!")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API KEYS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_API_KEY     = os.environ.get("GEMINI_API_KEY", "")
GROQ_API_KEY       = os.environ.get("GROQ_API_KEY", "")
ELEVENLABS_API_KEY = os.environ.get("ELEVENLABS_API_KEY", "")
GOOGLE_TTS_KEY     = os.environ.get("GOOGLE_TTS_KEY", "")
ANTHROPIC_API_KEY  = os.environ.get("ANTHROPIC_API_KEY", "")

# Paste your Uplift key inside the quotes below
# IMPORTANT: regenerate this at platform.upliftai.org/studio â€” the old one was shared publicly
UPLIFT_API_KEY = "sk_api_550f96150a184f649c70e053e3c3ebdd096ed4275130b995bb5b0ab264aa2478"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  UPLIFT AI VOICE CATALOGUE
#  Confirm exact IDs in your dashboard at platform.upliftai.org
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
UPLIFT_VOICES = [
    {
        "voice_id": "v_8eelc901",
        "label":    "Uplift AI Orator â€” Education (Male, Clear)",
        "engine":   "uplift",
    },
    {
        "voice_id": "v_meklc281",
        "label":    "Uplift AI Orator â€” Narrator (Female, Warm)",
        "engine":   "uplift",
    },
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TRANSLITERATION DICTIONARY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ENGLISH_TO_URDU_SCRIPT = {
    "subscribe":        "Ø³Ø¨Ø³Ú©Ø±Ø§Ø¦Ø¨",
    "subscribed":       "Ø³Ø¨Ø³Ú©Ø±Ø§Ø¦Ø¨ Ú©Ø± Ù„ÛŒØ§",
    "unsubscribe":      "Ø§Ù† Ø³Ø¨Ø³Ú©Ø±Ø§Ø¦Ø¨",
    "notification":     "Ù†ÙˆÙ¹ÛŒÙÚ©ÛŒØ´Ù†",
    "notifications":    "Ù†ÙˆÙ¹ÛŒÙÚ©ÛŒØ´Ù†Ø²",
    "bell":             "Ø¨ÛŒÙ„",
    "channel":          "Ú†ÛŒÙ†Ù„",
    "video":            "ÙˆÛŒÚˆÛŒÙˆ",
    "videos":           "ÙˆÛŒÚˆÛŒÙˆØ²",
    "live":             "Ù„Ø§Ø¦ÛŒÙˆ",
    "like":             "Ù„Ø§Ø¦ÛŒÚ©",
    "comment":          "Ú©Ù…Ù†Ù¹",
    "comments":         "Ú©Ù…Ù†Ù¹Ø³",
    "share":            "Ø´ÛŒØ¦Ø±",
    "playlist":         "Ù¾Ù„Û’ Ù„Ø³Ù¹",
    "update":           "Ø§Ù¾ÚˆÛŒÙ¹",
    "updates":          "Ø§Ù¾ÚˆÛŒÙ¹Ø³",
    "upload":           "Ø§Ù¾Ù„ÙˆÚˆ",
    "link":             "Ù„Ù†Ú©",
    "links":            "Ù„Ù†Ú©Ø³",
    "description":      "ÚˆØ³Ú©Ø±Ù¾Ø´Ù†",
    "weekly":           "ÛÙØªÛ ÙˆØ§Ø±",
    "daily":            "Ø±ÙˆØ²Ø§Ù†Û",
    "monthly":          "Ù…Ø§ÛØ§Ù†Û",
    "internet":         "Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹",
    "mobile":           "Ù…ÙˆØ¨Ø§Ø¦Ù„",
    "phone":            "ÙÙˆÙ†",
    "app":              "Ø§ÛŒÙ¾",
    "apps":             "Ø§ÛŒÙ¾Ø³",
    "online":           "Ø¢Ù† Ù„Ø§Ø¦Ù†",
    "offline":          "Ø¢Ù Ù„Ø§Ø¦Ù†",
    "password":         "Ù¾Ø§Ø³ ÙˆØ±Úˆ",
    "email":            "Ø§ÛŒ Ù…ÛŒÙ„",
    "website":          "ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹",
    "download":         "ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ",
    "screen":           "Ø§Ø³Ú©Ø±ÛŒÙ†",
    "button":           "Ø¨Ù¹Ù†",
    "lesson":           "Ø³Ø¨Ù‚",
    "lessons":          "Ø§Ø³Ø¨Ø§Ù‚",
    "class":            "Ú©Ù„Ø§Ø³",
    "course":           "Ú©ÙˆØ±Ø³",
    "tutorial":         "Ù¹ÛŒÙˆÙ¹ÙˆØ±ÛŒÙ„",
    "quiz":             "Ú©ÙˆØ¦Ø²",
    "test":             "Ù¹ÛŒØ³Ù¹",
    "topic":            "Ù¹Ø§Ù¾Ú©",
    "episode":          "Ø§ÛŒÙ¾ÛŒØ³ÙˆÚˆ",
    "series":           "Ø³ÛŒØ±ÛŒØ²",
    "turn on":          "Ø¢Ù† Ú©Ø±ÛŒÚº",
    "turn off":         "Ø¢Ù Ú©Ø±ÛŒÚº",
    "click":            "Ú©Ù„Ú©",
    "tap":              "Ù¹ÛŒÙ¾",
    "miss":             "Ù…Ø³",
    "never miss":       "Ú©Ø¨Ú¾ÛŒ Ù†Û Ù…ÙØ³Ø³ Ú©Ø±ÛŒÚº",
    "christian revive": "Ú©Ø±Ø³Ú†Ù† Ø±ÛŒÙˆØ§Ø¦ÛŒÙˆ",
    "christian":        "Ú©Ø±Ø³Ú†Ù†",
    "revive":           "Ø±ÛŒÙˆØ§Ø¦ÛŒÙˆ",
    "church":           "Ú†Ø±Ú†",
    "bible":            "Ø¨Ø§Ø¦Ø¨Ù„",
    "gospel":           "Ú¯Ø§Ø³Ù¾Ù„",
    "prayer":           "Ù¾Ø±ÛŒØ¦Ø±",
    "ministry":         "Ù…Ù†Ø³Ù¹Ø±ÛŒ",
    "worship":          "ÙˆØ±Ø´Ù¾",
    "team":             "Ù¹ÛŒÙ…",
    "page":             "Ù¾ÛŒØ¬",
    "profile":          "Ù¾Ø±ÙˆÙØ§Ø¦Ù„",
    "support":          "Ø³Ù¾ÙˆØ±Ù¹",
    "content":          "Ú©Ø§Ù†Ù¹ÛŒÙ†Ù¹",
    "format":           "ÙØ§Ø±Ù…ÛŒÙ¹",
    "platform":         "Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…",
    "social media":     "Ø³ÙˆØ´Ù„ Ù…ÛŒÚˆÛŒØ§",
    "facebook":         "ÙÛŒØ³ Ø¨Ú©",
    "instagram":        "Ø§Ù†Ø³Ù¹Ø§Ú¯Ø±Ø§Ù…",
    "youtube":          "ÛŒÙˆÙ¹ÛŒÙˆØ¨",
    "twitter":          "Ù¹ÙˆØ¦Ù¹Ø±",
    "whatsapp":         "ÙˆØ§Ù¹Ø³ Ø§ÛŒÙ¾",
}

def apply_transliteration(text: str) -> str:
    sorted_keys = sorted(ENGLISH_TO_URDU_SCRIPT.keys(), key=len, reverse=True)
    result = text
    for eng in sorted_keys:
        pattern = re.compile(re.escape(eng), re.IGNORECASE)
        result  = pattern.sub(ENGLISH_TO_URDU_SCRIPT[eng], result)
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AI REFINEMENT PROMPTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NATURALIZE_PROMPT = """You are a Pakistani Urdu language expert for TTS (text-to-speech) audio.

The text you receive is already partially in Urdu script. Your job:

1. Make it sound like natural Pakistani Urdu â€” warm, conversational, like a Pakistani YouTuber or narrator
2. If you see any remaining English words NOT in Urdu script, convert them to how Pakistanis pronounce them written in Urdu script
3. Keep the exact same meaning â€” do not add or remove information
4. Use proper Urdu punctuation: Û” for full stopØŒ for comma â€” these create pauses in TTS
5. Sentences should be short (10-12 words max) â€” TTS sounds better with short sentences
6. Do NOT use heavy Arabic/Persian Urdu â€” use everyday Pakistan Urdu

Return ONLY the final Urdu text in Urdu script. Nothing else."""

TTS_FINAL_PROMPT = """You are a TTS audio director for Pakistani Urdu.

You receive Urdu text that will be spoken by a voice engine. Your final task:

1. Read through every single word â€” if ANY word is still in Latin/English script, convert it to Urdu script
2. Add ØŒ after phrases where a speaker would naturally pause mid-sentence
3. Ensure Û” ends every complete sentence
4. Replace any word that sounds unnatural when spoken with a better alternative
5. The final audio should sound like a warm, professional Pakistani narrator

Return ONLY the final perfected Urdu text. No explanations. No English."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AI BOT CALLERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def call_gemini(text: str, system: str) -> str:
    if not GEMINI_API_KEY:
        return text
    try:
        r = requests.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}",
            headers={"Content-Type": "application/json"},
            json={
                "system_instruction": {"parts": [{"text": system}]},
                "contents": [{"parts": [{"text": text}]}],
                "generationConfig": {"maxOutputTokens": 2048, "temperature": 0.15}
            }, timeout=30
        )
        r.raise_for_status()
        result = r.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
        print(f"   âœ… Gemini: {result[:80]}...")
        return result
    except Exception as e:
        print(f"   âš ï¸  Gemini failed: {e}")
        return text


def call_groq(text: str, system: str) -> str:
    if not GROQ_API_KEY:
        return text
    try:
        r = requests.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers={"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"},
            json={
                "model": "llama-3.1-70b-versatile",
                "max_tokens": 2048,
                "temperature": 0.15,
                "messages": [
                    {"role": "system", "content": system},
                    {"role": "user",   "content": text}
                ]
            }, timeout=30
        )
        r.raise_for_status()
        result = r.json()["choices"][0]["message"]["content"].strip()
        print(f"   âœ… Groq/Llama: {result[:80]}...")
        return result
    except Exception as e:
        print(f"   âš ï¸  Groq failed: {e}")
        return text


def call_claude(text: str, system: str) -> str:
    if not ANTHROPIC_API_KEY:
        return text
    try:
        r = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": ANTHROPIC_API_KEY,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json"
            },
            json={
                "model": "claude-haiku-20240307",
                "max_tokens": 2048,
                "system": system,
                "messages": [{"role": "user", "content": text}]
            }, timeout=30
        )
        r.raise_for_status()
        result = r.json()["content"][0]["text"].strip()
        print(f"   âœ… Claude: {result[:80]}...")
        return result
    except Exception as e:
        print(f"   âš ï¸  Claude failed: {e}")
        return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  FULL TEXT PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_text_pipeline(english_text: str, target_lang: str = "ur") -> dict:
    stages = {}

    print("\nğŸŒ Stage 0: Google Translate...")
    sentences = re.split(r'(?<=[.!?"])\s+', english_text.strip())
    translated_parts = []
    for s in sentences:
        if not s.strip(): continue
        try:
            t = GoogleTranslator(source="auto", target=target_lang).translate(s.strip())
            translated_parts.append(t)
        except Exception:
            translated_parts.append(s)
    google_text = " ".join(translated_parts)
    stages["stage_0_google"] = google_text
    print(f"   â†’ {google_text[:100]}...")

    if target_lang != "ur":
        return {"final": google_text, "stages": stages}

    print("\nğŸ“– Stage 1: Dictionary transliteration (English â†’ Urdu script)...")
    transliterated = apply_transliteration(google_text)
    stages["stage_1_transliterate"] = transliterated
    print(f"   â†’ {transliterated[:100]}...")

    print("\nğŸ¤– Stage 2: AI naturalization...")
    if GEMINI_API_KEY:
        naturalized = call_gemini(transliterated, NATURALIZE_PROMPT)
    elif GROQ_API_KEY:
        naturalized = call_groq(transliterated, NATURALIZE_PROMPT)
    elif ANTHROPIC_API_KEY:
        naturalized = call_claude(transliterated, NATURALIZE_PROMPT)
    else:
        naturalized = transliterated
    stages["stage_2_naturalized"] = naturalized

    print("\nğŸ¤– Stage 3: TTS optimization...")
    if GROQ_API_KEY:
        optimized = call_groq(naturalized, TTS_FINAL_PROMPT)
    elif GEMINI_API_KEY:
        optimized = call_gemini(naturalized, TTS_FINAL_PROMPT)
    else:
        optimized = naturalized
    stages["stage_3_tts_ready"] = optimized

    print("\nğŸ” Stage 4: Final Latin character cleanup...")
    if re.search(r'[a-zA-Z]{2,}', optimized):
        print("   Found remaining Latin text â€” running extra transliteration pass...")
        optimized = apply_transliteration(optimized)
    stages["stage_4_final"] = optimized

    print(f"\nâœ¨ FINAL TEXT: {optimized}")
    return {"final": optimized, "stages": stages}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TTS ENGINE: UPLIFT AI ORATOR
#  âœ… Speed slider NOW works
#  âŒ Pitch slider has no effect (Uplift API doesn't support pitch)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_ffmpeg_speed_pitch(input_path: str, output_path: str,
                              rate: int = 0, pitch: int = 0) -> bool:
    """
    Post-process audio with ffmpeg to apply speed and pitch adjustments.
    This works on ANY engine including Uplift AI.

    rate:  -30 to +30  â†’  atempo 0.5x to 2.0x
    pitch: -20 to +20  â†’  asetrate (shifts pitch via sample rate trick)

    ffmpeg atempo only supports 0.5â€“2.0 range per filter, so we chain two
    filters for extreme values (e.g. 0.25x = atempo=0.5,atempo=0.5).
    """
    import subprocess

    if rate == 0 and pitch == 0:
        # Nothing to do â€” just rename
        import shutil
        shutil.move(input_path, output_path)
        return True

    # Map rate -30â€¦+30 â†’ tempo 0.5â€¦2.0
    tempo = round(1.0 + (rate / 30.0) * 1.0, 3)
    tempo = max(0.25, min(4.0, tempo))

    # Map pitch -20â€¦+20 â†’ semitone shift (-6 to +6 semitones feels natural)
    semitones = round((pitch / 20.0) * 6.0, 2)

    # Build ffmpeg filter chain
    filters = []

    # Pitch shift via asetrate + atempo compensation
    if pitch != 0:
        base_rate = 22050
        shifted_rate = int(base_rate * (2 ** (semitones / 12.0)))
        filters.append(f"asetrate={shifted_rate}")
        filters.append(f"aresample={base_rate}")
        # compensate tempo change caused by asetrate
        pitch_tempo_comp = base_rate / shifted_rate
        effective_tempo = tempo * pitch_tempo_comp
    else:
        effective_tempo = tempo

    # atempo only supports 0.5â€“2.0 per filter â€” chain for extreme values
    effective_tempo = max(0.25, min(4.0, effective_tempo))
    if effective_tempo < 0.5:
        filters.append(f"atempo={effective_tempo*2:.3f},atempo=0.5")
    elif effective_tempo > 2.0:
        filters.append(f"atempo=2.0,atempo={effective_tempo/2:.3f}")
    else:
        filters.append(f"atempo={effective_tempo:.3f}")

    filter_str = ",".join(filters)

    try:
        result = subprocess.run([
            "ffmpeg", "-i", input_path,
            "-filter:a", filter_str,
            "-c:a", "libmp3lame", "-q:a", "2",
            output_path, "-y"
        ], capture_output=True, text=True, timeout=30)

        if result.returncode == 0:
            print(f"   âœ… ffmpeg speed/pitch: tempo={effective_tempo:.2f} semitones={semitones}")
            # Clean up original
            if os.path.exists(input_path) and input_path != output_path:
                os.remove(input_path)
            return True
        else:
            print(f"   âš ï¸  ffmpeg failed: {result.stderr[-200:]}")
            # Fall back to original unprocessed file
            import shutil
            shutil.move(input_path, output_path)
            return True  # still return True â€” we have audio, just unprocessed
    except Exception as e:
        print(f"   âš ï¸  ffmpeg speed/pitch error: {e}")
        import shutil
        shutil.move(input_path, output_path)
        return True


def tts_uplift(text: str, output_path: str, voice_id: str = "v_8eelc901",
               rate: int = 0, pitch: int = 0) -> bool:
    """
    Uplift AI Orator â€” downloads raw audio then applies speed+pitch via ffmpeg.
    This gives us REAL speed and pitch control regardless of what the Uplift API supports.
    """
    if not UPLIFT_API_KEY:
        print("   âš ï¸  UPLIFT_API_KEY not set â€” skipping Uplift AI")
        return False

    raw_path = output_path + ".raw.mp3"

    try:
        r = requests.post(
            "https://api.upliftai.org/v1/synthesis/text-to-speech",
            headers={
                "Authorization": f"Bearer {UPLIFT_API_KEY}",
                "Content-Type":  "application/json"
            },
            json={
                "voiceId":      voice_id,
                "text":         text,
                "outputFormat": "MP3_22050_128"
            },
            timeout=60
        )
        if r.status_code == 200:
            with open(raw_path, "wb") as f:
                f.write(r.content)
            print(f"   âœ… Uplift AI ({voice_id}): downloaded raw audio")
            # Apply speed + pitch via ffmpeg post-processing
            return apply_ffmpeg_speed_pitch(raw_path, output_path, rate=rate, pitch=pitch)

        print(f"   âš ï¸  Uplift AI error {r.status_code}: {r.text[:200]}")
        return False

    except Exception as e:
        print(f"   âš ï¸  Uplift AI failed: {e}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TTS ENGINE: ELEVENLABS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ELEVENLABS_VOICES = {
    "adam":   "pNInz6obpgDQGcFmaJgB",
    "Antoni": "ErXwobaYiN019PkySvjV",
    "josh":   "TxGEqnHWrfWFTfGW9XjX",
    "bella":  "EXAVITQu4vr4xnSDxMaL",
    "elli":   "MF3mGyEYCl7XYWbV9V6O",
}

def tts_elevenlabs(text: str, output_path: str, voice_key: str = "adam") -> bool:
    if not ELEVENLABS_API_KEY:
        return False
    voice_id = ELEVENLABS_VOICES.get(voice_key, ELEVENLABS_VOICES["adam"])
    try:
        r = requests.post(
            f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}",
            headers={"xi-api-key": ELEVENLABS_API_KEY, "Content-Type": "application/json"},
            json={
                "text": text,
                "model_id": "eleven_multilingual_v2",
                "voice_settings": {
                    "stability": 0.40,
                    "similarity_boost": 0.80,
                    "style": 0.30,
                    "use_speaker_boost": True
                }
            }, timeout=60
        )
        if r.status_code == 200:
            with open(output_path, "wb") as f: f.write(r.content)
            print(f"   âœ… ElevenLabs ({voice_key}): saved")
            return True
        print(f"   âš ï¸  ElevenLabs error {r.status_code}: {r.text[:150]}")
        return False
    except Exception as e:
        print(f"   âš ï¸  ElevenLabs failed: {e}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TTS ENGINE: GOOGLE CLOUD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tts_google_cloud(text: str, output_path: str, gender: str = "MALE") -> bool:
    if not GOOGLE_TTS_KEY:
        return False
    try:
        r = requests.post(
            f"https://texttospeech.googleapis.com/v1/text:synthesize?key={GOOGLE_TTS_KEY}",
            headers={"Content-Type": "application/json"},
            json={
                "input": {"text": text},
                "voice": {"languageCode": "ur-PK", "ssmlGender": gender},
                "audioConfig": {
                    "audioEncoding": "MP3",
                    "speakingRate": 0.92,
                    "pitch": 0.0,
                    "effectsProfileId": ["headphone-class-device"]
                }
            }, timeout=30
        )
        r.raise_for_status()
        audio_b64 = r.json().get("audioContent", "")
        if audio_b64:
            with open(output_path, "wb") as f: f.write(base64.b64decode(audio_b64))
            print(f"   âœ… Google Cloud TTS: saved")
            return True
        return False
    except Exception as e:
        print(f"   âš ï¸  Google Cloud TTS failed: {e}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TTS ENGINE: MICROSOFT EDGE TTS (free fallback)
#  âœ… Both speed AND pitch sliders work
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _edge_save(text, voice, output_path, rate, pitch):
    c = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)
    await c.save(output_path)

def tts_edge(text: str, output_path: str, voice: str = "ur-PK-AsadNeural",
             rate: str = "-5%", pitch: str = "+0Hz") -> bool:
    try:
        asyncio.run(_edge_save(text, voice, output_path, rate, pitch))
        print(f"   âœ… Edge TTS ({voice}): saved")
        return True
    except Exception as e:
        print(f"   âš ï¸  Edge TTS failed: {e}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AUDIO CHUNKING + MERGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_text(text: str, max_chars: int = 400) -> list:
    sentences = re.split(r'(?<=[Û”.!?])\s+', text)
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) <= max_chars:
            current += " " + s
        else:
            if current.strip(): chunks.append(current.strip())
            current = s
    if current.strip(): chunks.append(current.strip())
    return chunks if chunks else [text]


def merge_audio_chunks(chunk_files: list, output_path: str):
    if len(chunk_files) == 1:
        os.rename(chunk_files[0], output_path)
    else:
        import subprocess
        list_file = f"/tmp/list_{uuid.uuid4().hex}.txt"
        with open(list_file, "w") as f:
            for cf in chunk_files: f.write(f"file '{cf}'\n")
        subprocess.run([
            "ffmpeg", "-f", "concat", "-safe", "0",
            "-i", list_file, "-c", "copy", output_path, "-y"
        ], check=True, capture_output=True)
        os.remove(list_file)
        for cf in chunk_files:
            if os.path.exists(cf): os.remove(cf)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN VOICE GENERATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_all_voices(text: str, rate: int = -5, pitch: int = 0) -> list:
    rate_str  = f"{'+' if rate >= 0 else ''}{rate}%"
    pitch_str = f"{'+' if pitch >= 0 else ''}{pitch}Hz"
    chunks    = split_text(text, max_chars=400)
    results   = []

    voices_to_generate = []

    # 1. Uplift AI â€” BEST quality
    if UPLIFT_API_KEY:
        for v in UPLIFT_VOICES:
            voices_to_generate.append({**v})

    # 2. ElevenLabs
    if ELEVENLABS_API_KEY:
        voices_to_generate += [
            {"engine": "elevenlabs", "voice_key": "adam",   "label": "ElevenLabs â€” Adam (Male, Warm)"},
            {"engine": "elevenlabs", "voice_key": "Antoni", "label": "ElevenLabs â€” Antoni (Male, Deep)"},
        ]

    # 3. Google Cloud TTS
    if GOOGLE_TTS_KEY:
        voices_to_generate += [
            {"engine": "google", "gender": "MALE",   "label": "Google Cloud â€” Male (Pakistani Urdu)"},
            {"engine": "google", "gender": "FEMALE", "label": "Google Cloud â€” Female (Pakistani Urdu)"},
        ]

    # 4. Edge TTS â€” always available free fallback
    voices_to_generate += [
        {"engine": "edge", "voice": "ur-PK-AsadNeural", "label": "Microsoft Edge â€” Asad (Male)"},
        {"engine": "edge", "voice": "ur-PK-UzmaNeural", "label": "Microsoft Edge â€” Uzma (Female)"},
    ]

    print(f"\nğŸ”Š Generating {len(voices_to_generate)} voice versions...")

    for v in voices_to_generate:
        engine      = v["engine"]
        label       = v["label"]
        output_path = f"/tmp/voice_{uuid.uuid4().hex}.mp3"
        chunk_files = []
        success     = False

        print(f"\n   Generating: {label}")

        for chunk in chunks:
            chunk_path = f"/tmp/chunk_{uuid.uuid4().hex}.mp3"

            if engine == "uplift":
                # âœ… rate is now passed through to Uplift
                ok = tts_uplift(chunk, chunk_path, v.get("voice_id", "v_8eelc901"), rate=rate, pitch=pitch)
                if not ok:
                    ok = tts_edge(chunk, chunk_path, "ur-PK-AsadNeural", rate_str, pitch_str)
                success = ok

            elif engine == "elevenlabs":
                ok = tts_elevenlabs(chunk, chunk_path, v.get("voice_key", "adam"))
                if not ok:
                    ok = tts_edge(chunk, chunk_path, "ur-PK-AsadNeural", rate_str, pitch_str)
                success = ok

            elif engine == "google":
                ok = tts_google_cloud(chunk, chunk_path, v.get("gender", "MALE"))
                if not ok:
                    ok = tts_edge(chunk, chunk_path, "ur-PK-AsadNeural", rate_str, pitch_str)
                success = ok

            else:  # edge â€” supports both rate AND pitch
                success = tts_edge(chunk, chunk_path, v.get("voice", "ur-PK-AsadNeural"),
                                   rate_str, pitch_str)

            if success and os.path.exists(chunk_path):
                chunk_files.append(chunk_path)

        if chunk_files:
            try:
                merge_audio_chunks(chunk_files, output_path)
                token = uuid.uuid4().hex
                app.config[f"audio_{token}"] = output_path
                results.append({
                    "engine":    engine,
                    "label":     label,
                    "audio_url": f"/audio?t={token}",
                    "token":     token
                })
            except Exception as e:
                print(f"   âš ï¸  Merge failed for {label}: {e}")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ROUTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/generate", methods=["POST"])
def generate():
    try:
        text        = request.form["text"]
        rate        = int(request.form.get("rate", -5))
        pitch       = int(request.form.get("pitch", 0))
        target_lang = request.form.get("lang", "ur")

        pipeline    = process_text_pipeline(text, target_lang=target_lang)
        final_text  = pipeline["final"]
        stages      = pipeline["stages"]

        if not final_text.strip():
            return jsonify({"success": False, "error": "Processing failed"}), 400

        voices = generate_all_voices(final_text, rate=rate, pitch=pitch)

        best_voice     = voices[0] if voices else None
        best_audio_url = best_voice["audio_url"] if best_voice else ""

        return jsonify({
            "success":         True,
            "final_text":      final_text,
            "translated_text": final_text,
            "audio_url":       best_audio_url,
            "voices":          voices,
            "pipeline":        stages
        })

    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/audio")
def audio():
    token    = request.args.get("t", "")
    filename = app.config.get(f"audio_{token}", "")
    if not filename or not os.path.exists(filename):
        return "File not found", 404
    return send_file(filename, mimetype="audio/mpeg")


@app.route("/voices_status")
def voices_status():
    return jsonify({
        "uplift_ai":  {
            "active":       bool(UPLIFT_API_KEY),
            "speed_slider": "âœ… works",
            "pitch_slider": "âŒ not supported by Uplift API",
            "quality":      "ğŸ¥‡ Best â€” native Pakistani voices",
        },
        "edge_tts": {
            "active":       True,
            "speed_slider": "âœ… works",
            "pitch_slider": "âœ… works",
            "quality":      "ğŸ†“ Free fallback",
        },
        "elevenlabs": {"active": bool(ELEVENLABS_API_KEY)},
        "google_tts": {"active": bool(GOOGLE_TTS_KEY)},
        "gemini_ai":  {"active": bool(GEMINI_API_KEY)},
        "groq_ai":    {"active": bool(GROQ_API_KEY)},
    })


@app.route("/transcribe", methods=["POST"])
def transcribe():
    if "video" not in request.files:
        return jsonify({"error": "No file uploaded"})
    uploaded   = request.files["video"]
    ext        = os.path.splitext(uploaded.filename.lower())[1]
    base_id    = uuid.uuid4().hex
    input_path = f"/tmp/upload_{base_id}{ext}"
    audio_path = f"/tmp/upload_{base_id}.wav"
    uploaded.save(input_path)
    try:
        import subprocess
        res = subprocess.run([
            "ffmpeg", "-i", input_path,
            "-ar", "16000", "-ac", "1",
            "-c:a", "pcm_s16le", audio_path, "-y"
        ], capture_output=True, text=True)
        if res.returncode != 0:
            return jsonify({"error": f"FFmpeg: {res.stderr}"})
        segments, _ = model.transcribe(audio_path)
        transcript  = " ".join([s.text for s in segments])
        return jsonify({"transcript": transcript})
    except Exception as e:
        return jsonify({"error": str(e)})
    finally:
        if os.path.exists(input_path): os.remove(input_path)
        if os.path.exists(audio_path): os.remove(audio_path)


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)